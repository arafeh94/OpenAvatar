# RAG Chat Service with Document Enhancement

A Retrieval-Augmented Generation (RAG) chat service powered by LangChain, ChromaDB, and OpenAI/Ollama. Enhances user
queries with AI-generated keywords/summaries and retrieves context from embedded documents for accurate responses.

## Features

- **PDF Document Embedding**: Process and split PDFs into searchable text chunks.
- **AI-Powered Enhancements**:
    - Keyword extraction and refinement using RAKE and LLMs.
    - Context-aware summarization of documents.
    - Query enhancement using extracted metadata.
- **Multi-Model Support**: Choose between OpenAI GPT-4o or Ollama models (e.g., Gemma2).
- **Stateful Conversations**: Track chat history and context with LangGraph.
- **Configurable Workflows**: Customizable prompts and retrieval parameters.

## Installation

NLTK Setup:

```python
from services.rag import ChatService

ChatService.setup_nltk()  # Downloads required NLP Tokens
```

# Usage

### ChatService Creation

```python
service = ChatService.create()  # Default (Uses OpenAI)
service = ChatService.create_openai(model_name="gpt-4o")  # Using OpenAI (API calls)
service = ChatService.create_ollama(model_name="gemma2:2b")  # Using Ollama (local calls)
```

### Embedding PDF

```python
service.embed_pdf("./files/1.pdf")
```

### Extracting Relevant Information - AI-Based Keyword Extraction and Summary

Extract keywords and generate a summary using AI:

```python
ai_enhance = chat_service.get_ai_keywords_summary(docs, written_summary)
```

- **Keyword Extraction**: Keywords are extracted using NLTK's RAKE algorithm and refined by the AI to select the most
  relevant ones.
- **Summary Generation**: A new summary is generated by combining the provided written summary with the extracted
  keywords, to generate an accurate and context-aware output.

# Create a New Chat Session

Create a new chat session with query enhancement enabled:

```python
chat = service.new_chat(improve_input=True, v=2)
```

## Improve user queries using `improve_input`

When `improve_input=True`, the following enhancements are applied:

- **Query Refinement**: The input question is improved using the provided enhancements (e.g., keywords, summaries),
  leading to higher-quality document retrieval.
- **System Prompt Enhancement**: Enhancements are added to the system prompt, aiding the AI in generating more accurate
  and contextually relevant responses.

## Change LangChain Graph Versions Using `v`

There are currently two versions used to compile the LangChain graph:

- **Version 1: Single Retrieval Prompt String (v=1)**
    - Compile the context as a **single retrieval prompt string** and provide it to the AI.

- **Version 2: Context as Messages (v=2)**
    - Supply the context as **messages** within the AI system. It is supplied to the invoke like:

      ```python
      [(system, system_message), (ai, ai_answer), (human, human_input)]
      ```

### Ask a Question with Source Filtering and Enhancements

```python
response = chat.answer(
    # User query  
    "What are the key findings?",
    # Optional: Filter results by specific document source
    source=["./files/1.pdf"],
    # Optional: Provide context for better results
    enhancements={"summary": "Climate study 2023", "keywords": "climate, study"}
)
```

### Enhancements

- **Purpose 1**: They refine the question and improve the quality of the retrieved documents and final response.
- **Purpose 2**: They are added as extra context to help the AI improve its output.
- **Condition**: These are only included if `improve_input=True` when creating the chat session.

### Response Generation Flow

- **Input Enhancement**: Refines query using AI-generated keywords/summaries (can be disabled using `improve_input`).
- **Document Retrieval**: Finds relevant text chunks from the vector store.
- **Context Assembly**: Combines chat history, documents, and enhancements.
- **LLM Generation**: Produces final response using the configured model.

## Configuration

### Prompts

Modify `AIPrompts` class for:

- **Keyword Refinement** (`IMPROVE_KEYWORDS`)
- **Summary Generation** (`GENERATE_SUMMARY`)
- **Query Enhancement** (`IMPROVE_INPUT`)
- **RAG Behavior** (`RETRIEVAL_DEFAULT`)

### Creation

- VectorStore Persistent storage using **Chroma**.
- Switch embeddings via `create_openai/create_ollama`.
- Adjust chunking with **RecursiveCharacterTextSplitter**.

